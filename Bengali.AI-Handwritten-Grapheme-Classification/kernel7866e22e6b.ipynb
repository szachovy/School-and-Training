{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np\nimport pandas as pd\nimport random\n\nimport matplotlib.pyplot as plt\nfrom PIL import Image, ImageDraw, ImageFont\nfrom cv2 import resize, cvtColor, COLOR_GRAY2RGB, INTER_AREA\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport seaborn as sns\n\nfrom tqdm import tqdm\n# from numba import cuda\n\nfrom albumentations.core.composition import Compose\nfrom albumentations.core.transforms_interface import ImageOnlyTransform\nfrom albumentations.augmentations.functional import *\n\nfrom keras.callbacks import ReduceLROnPlateau\nfrom sklearn.metrics import confusion_matrix\n# from keras.models import Model\n# from keras.layers import Input, Dense, Conv2D, BatchNormalization, MaxPool2D, Dropout, Flatten\n\nfrom torch import nn\nfrom torch import device\nfrom torch import tensor\nfrom torch import load, save\nfrom torch import dtype\nimport torch # TODO: find a method to import tensor.float dtype\nimport torch.nn.functional as F\n\nfrom torchvision.models import resnet34, densenet121\nfrom torch.utils import data\nfrom torch.optim import Adam\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\n\nrandom.seed(42)\ntorch.cuda.seed_all()\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. Data Collection"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/bengaliai-cv19/train.csv') \ntest = pd.read_csv('/kaggle/input/bengaliai-cv19/test.csv')\nclass_map = pd.read_csv('/kaggle/input/bengaliai-cv19/class_map.csv')\nsample_submission = pd.read_csv('/kaggle/input/bengaliai-cv19/sample_submission.csv')\n\n# later do for all parquet files\ntrain_0 = pd.read_parquet('/kaggle/input/bengaliai-cv19/train_image_data_0.parquet')\n# train_1 = pd.read_parquet('/kaggle/input/bengaliai-cv19/train_image_data_1.parquet')\n# train_2 = pd.read_parquet('/kaggle/input/bengaliai-cv19/train_image_data_2.parquet')\n# train_3 = pd.read_parquet('/kaggle/input/bengaliai-cv19/train_image_data_3.parquet')\n\ntest_0 = pd.read_parquet('/kaggle/input/bengaliai-cv19/test_image_data_0.parquet')\ntest_1 = pd.read_parquet('/kaggle/input/bengaliai-cv19/test_image_data_1.parquet')\ntest_2 = pd.read_parquet('/kaggle/input/bengaliai-cv19/test_image_data_2.parquet')\ntest_3 = pd.read_parquet('/kaggle/input/bengaliai-cv19/test_image_data_3.parquet')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class_map.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_0.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('train shape', train.shape)\nprint('test shape', test.shape)\nprint('class_map shape', class_map.shape)\nprint('train_0 shape', train_0.shape)\nprint('test_0 shape', test_0.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def display_image_from_pixels(data, subplots_size=5, smaller_data_alert=False):\n    plt.figure()\n    fig, ax = plt.subplots(subplots_size, subplots_size, figsize=(12,12))\n\n    for i, index in enumerate(data.index):\n        image_id = data.iloc[i]['image_id']\n        image = data.iloc[i].drop('image_id').values.astype(np.uint8)\n        image = Image.fromarray(image.reshape(137, 236))\n        \n        ax[i//subplots_size, i%subplots_size].imshow(image)\n        ax[i//subplots_size, i%subplots_size].set_title(image_id)\n        ax[i//subplots_size, i%subplots_size].axis('off')\n        \n    if smaller_data_alert:\n        for empty_subplot in range(3, 25):\n            ax[empty_subplot//subplots_size, empty_subplot%subplots_size].set_visible(False)\n            \n\ndisplay_image_from_pixels(train_0.sample(25))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display_image_from_pixels(test_0, smaller_data_alert=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Exploratory Data Analysis (EDA)"},{"metadata":{},"cell_type":"markdown","source":"### 2.1. Show all possible class map signs"},{"metadata":{"trusted":true},"cell_type":"code","source":"def show_class_maps():\n    print('-----------------')\n    print('grapheme_root')\n    print(class_map.loc[class_map['component_type'] == 'grapheme_root']['component'].values)\n    print('-----------------')\n    print('map_vowel')\n    print(class_map.loc[class_map['component_type'] == 'vowel_diacritic']['component'].values)\n    print('-----------------')\n    print('map_diacritic')\n    print(class_map.loc[class_map['component_type'] == 'consonant_diacritic']['component'].values)\n    \nshow_class_maps()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.2. Get frequency of class map occurrences in train set"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_freq(column='grapheme'):\n    col = train[column].value_counts().rename_axis(column).reset_index(name='count')\n    fig = px.bar(col, y='count', x=column)\n    fig.show()\n    \nplot_freq('grapheme')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_freq('grapheme_root')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_freq('vowel_diacritic')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_freq('consonant_diacritic')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.3. Plot feature occurrence dependencies (encoded)"},{"metadata":{"trusted":true},"cell_type":"code","source":"def features_heatmap(feature1, feature2, width, length):\n    df = train.groupby([feature1, feature2])['grapheme'].count().reset_index() \n    df = df.pivot(feature1, feature2, 'grapheme')\n    plt.figure(figsize=(width, length))\n    sns.heatmap(df, annot=True, fmt='3.0f', linewidths=.5, cmap='Blues')\n    \nfeatures_heatmap('vowel_diacritic','consonant_diacritic' ,12 , 4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#%%javascript\n#IPython.OutputArea.auto_scroll_threshold = 9999;","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features_heatmap('grapheme_root','consonant_diacritic', 12, 40)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features_heatmap('grapheme_root','vowel_diacritic', 18, 40)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Feature Engineering (FE)"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nclass FE(object):\n    __constraints__ = {'WIDTH': 137, 'HEIGHT': 236, 'END_SIZE': 128}\n\n    @staticmethod\n    def make_2d(dataset, vector):\n        image = dataset.iloc[vector].drop('image_id').values.astype(np.uint8)\n        image = image.reshape(FE.__constraints__['WIDTH'], FE.__constraints__['HEIGHT'])/1\n        return image\n\n    def crop_top(self, image, threshold):\n        idx = 0\n        for row in range(FE.__constraints__['WIDTH']):\n            if np.sum(image[row]) / 255 > threshold:\n                idx += 1\n            else:\n                return idx\n\n    def crop_bot(self, image, threshold):\n        idx = 0\n        for row in reversed(range(FE.__constraints__['WIDTH'])):\n            if np.sum(image[row]) / 255 > threshold:\n                idx += 1\n            else:\n                return FE.__constraints__['WIDTH'] - idx    \n\n    def crop_left(self, image, threshold):\n        idx = 0\n        for col in range(FE.__constraints__['HEIGHT']):\n            if np.sum(image[:, col]) / 255 > threshold-95:\n                idx += 1\n            else:\n                return idx\n\n    def crop_right(self, image, threshold):\n        idx = 0\n        for col in reversed(range(FE.__constraints__['HEIGHT'])):\n            if np.sum(image[:, col]) / 255 > threshold-95:\n                idx += 1\n            else:\n                return FE.__constraints__['HEIGHT'] - idx \n\n    def crop_resize_image(self, image, threshold=230):\n        return cv2.resize(image[self.crop_top(image, threshold): self.crop_bot(image, threshold), self.crop_left(image, threshold): self.crop_right(image, threshold)], (FE.__constraints__['END_SIZE'], FE.__constraints__['END_SIZE']), interpolation = INTER_AREA)\n\n    @staticmethod\n    def random_aug_mix(image, prob=0.5):\n        image = cvtColor(image.astype(np.uint8), COLOR_GRAY2RGB)\n        if prob > random.random():\n            image = add_fog(image, fog_coef=0.5, alpha_coef=0, haze_list=[])\n        if prob > random.random():\n            image = add_snow(image, snow_point=1, brightness_coeff=0.2)\n        if prob > random.random():\n            image = elastic_transform(image, alpha=8, sigma=1, alpha_affine=0.8, interpolation=1, border_mode=4, value=None, random_state=None, approximate=False)\n        if prob > random.random():\n            image = iso_noise(image, color_shift=5, intensity=5)\n        return np.moveaxis(image[:,:,:1], -1, 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.1. Cropping, Centering and Resizing images"},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.concat([test_0, test_1, test_2, test_3])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# prepare datasets (change dims)\nimages_test = np.zeros(((137, 236, 12)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for vector in tqdm(range(test.shape[0])):\n    images_test[:,:,vector] = FE.make_2d(test, vector)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# dims after resize\nresized_test = np.zeros(((128, 128, 12)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for vector in tqdm(range(test.shape[0])): \n    resized_test[:,:,vector] = FE().crop_resize_image(images_test[:,:,vector], threshold=230)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del images_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.3. AugMix train set calibration using albumentations"},{"metadata":{"trusted":true},"cell_type":"code","source":"# for torch\nX_test = np.zeros(((12, 1, 128, 128)))\n\n# for keras\n# X_train = np.zeros(((50210, 128, 128, 1)))\n# X_test = np.zeros(((12, 128, 128, 1)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for vector in tqdm(range(test.shape[0])):\n    X_test[vector,:,:,:] = FE.random_aug_mix(resized_test[:,:,vector], prob=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del resized_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.4. Merging previous steps into pipeline for future training sets preparation"},{"metadata":{"trusted":true},"cell_type":"code","source":"TRAIN_SIZE = 10000","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# @cuda.autojit\ndef train_preprocessing_pipeline(train_set, train_step):\n    print('{} train step processing'.format(train_step))\n    images_train = np.zeros(((137, 236, TRAIN_SIZE)))\n    \n    for vector in tqdm(range(TRAIN_SIZE)): #range(train_0.shape[0])\n        images_train[:,:,vector] = FE.make_2d(train_set, vector + (TRAIN_SIZE * train_step))\n        \n    resized_train = np.zeros(((128, 128, TRAIN_SIZE)))\n    \n    for vector in tqdm(range(TRAIN_SIZE)): #train_0.shape[0]\n        resized_train[:,:,vector] = FE().crop_resize_image(images_train[:,:,vector], threshold=230)\n        \n    del images_train\n    \n    X_train = np.zeros(((TRAIN_SIZE, 1, 128, 128)))\n    \n    for vector in tqdm(range(TRAIN_SIZE)): \n        X_train[vector,:,:,:] = FE.random_aug_mix(resized_train[:,:,vector], prob=0.3)\n    \n    del resized_train\n    \n    print('processed')\n    \n    return data.DataLoader(\n                        tensor(X_train),\n                        batch_size=BATCH_SIZE,\n                        num_workers=N_WORKERS,\n                        shuffle=False\n                    )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Modeling"},{"metadata":{"trusted":true},"cell_type":"code","source":"device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\ndevice","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.1. NN Architecture"},{"metadata":{"trusted":true},"cell_type":"code","source":"class ConvBlock(nn.Module):\n    def __init__(self, in_channels, out_channels):        \n        super(ConvBlock, self).__init__()\n        \n        self.conv2d1 = nn.Conv2d(in_channels=in_channels, out_channels=6, kernel_size=6, stride=2, padding=2)\n        self.batch_norm = nn.BatchNorm2d(num_features=6)\n        self.relu = nn.ReLU(True)\n        self.max_pooling =  nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n        self.conv2d2 = nn.Conv2d(in_channels=6, out_channels=out_channels, kernel_size=6, stride=2, padding=0)\n        \n    def forward(self, x):\n        x = self.conv2d1(x)\n        x = self.batch_norm(x)\n        x = self.relu(x)\n        x = self.max_pooling(x)\n        x = self.conv2d2(x)\n        return self.relu(x)\n    \nclass ResidualBlock(nn.Module):\n    def __init__(self): # prob\n        super(ResidualBlock, self).__init__()\n        self.id_block = nn.Sequential(\n                            nn.Conv2d(in_channels=8, out_channels=8, kernel_size=3, stride=1, padding=1),\n                            #nn.BatchNorm2d(num_features=8) \n                            nn.MaxPool2d(kernel_size=3, padding=1, stride=1)\n                            )\n        \n        self.skip = nn.Sequential()\n            \n    def forward(self, x):\n        residual = x\n        x = self.id_block(x)\n        x += self.skip(residual)\n        return nn.ReLU(True)(x)\n\n\nclass BottleNeck(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(BottleNeck, self).__init__()\n        self.bottle_neck = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, \n                                     kernel_size=1, stride=1, padding=0)\n\n    def forward(self, x): \n        return self.bottle_neck(x)\n\nclass ResNet(nn.Module):\n    def __init__(self):\n        super(ResNet, self).__init__()\n        \n#         self.backbone = resnet34(pretrained=False)\n        \n        self.conv_block = ConvBlock(in_channels=1, out_channels=16)\n        \n        self.bottle_neck = BottleNeck(in_channels=16, out_channels=8)\n        \n        self.res_blocks = ResidualBlock()\n        \n        self.average_pooling = nn.AvgPool2d(kernel_size=2, stride=2, padding=0)\n        \n        self.flatten = nn.Flatten()\n        self.norm = nn.BatchNorm2d(num_features=8)\n        \n        self.root = nn.Linear(392, 168)\n        self.vowel = nn.Linear(392, 11)\n        self.consonant = nn.Linear(392, 7)\n\n    def forward(self, x):\n        \n       # x = self.forward_backbone(x)\n        \n        x = self.conv_block(x)\n        x = self.bottle_neck(x)\n        \n        x = self.res_blocks(x)\n        x = self.res_blocks(x)\n        x = self.res_blocks(x)\n        x = self.res_blocks(x)\n        x = self.res_blocks(x)\n        x = self.res_blocks(x)\n        \n        x = self.res_blocks(x)\n        x = self.res_blocks(x)\n        x = self.res_blocks(x)\n        x = self.res_blocks(x)\n        x = self.res_blocks(x)\n        x = self.res_blocks(x)\n        \n        \n        x = self.average_pooling(x)\n        x = self.flatten(x)\n        \n        root = self.root(x)\n        vowel = self.vowel(x)\n        consonant = self.consonant(x)\n        \n        return root, vowel, consonant","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.2. Constraints setting"},{"metadata":{"trusted":true},"cell_type":"code","source":"BATCH_SIZE = TRAIN_SIZE // 20 #32\nN_WORKERS = 4\nN_EPOCHS = 10\nDEPLOYMENT_WEIGHTS = 'baseline_weights.pth'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.3. Model Template"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = ResNet().to(device)\ncriterion = nn.BCEWithLogitsLoss() # KLDivLoss()\noptimizer = Adam(model.parameters(), lr=0.5)\nscheduler = ReduceLROnPlateau(optimizer, 'min', min_lr=0.1, verbose=True) # patience=1000, min_lr=0.1, \nepoch_losses = []","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_nn(train_set, train_step, data_loader_train = []):\n\n    for epoch in range(N_EPOCHS):\n\n            print('Epoch {}/{}'.format(epoch, N_EPOCHS - 1))\n            print('-' * 10)\n\n            model.train()\n            tr_loss = 0  \n            \n            if epoch == 0:\n                data_loader_train = train_preprocessing_pipeline(train_set, train_step)\n                \n            for step, batch in enumerate(tqdm(data_loader_train)):\n                inputs = tensor(batch)\n                l_graph = tensor(Y_train_root[(batch.shape[0]*step):(batch.shape[0]*(step+1))])\n                l_vowel = tensor(Y_train_vowel[(batch.shape[0]*step):(batch.shape[0]*(step+1))])\n                l_conso = tensor(Y_train_consonant[(batch.shape[0]*step):(batch.shape[0]*(step+1))])\n\n                inputs = inputs.to(device, dtype=torch.float)\n                l_graph = l_graph.to(device, dtype=torch.float)\n                l_vowel = l_vowel.to(device, dtype=torch.float)\n                l_conso = l_conso.to(device, dtype=torch.float)\n\n                out_graph, out_vowel, out_conso = model(inputs)\n\n                loss_graph = criterion(out_graph, l_graph)\n                loss_vowel = criterion(out_vowel, l_vowel)\n                loss_conso = criterion(out_conso, l_conso)\n\n                loss = loss_graph + loss_vowel + loss_conso\n\n                scheduler.step(loss)\n                loss.backward()\n\n                tr_loss += loss.item()\n\n                optimizer.step()\n                optimizer.zero_grad()\n\n            epoch_losses.append(tr_loss / len(data_loader_train))\n            print('Training Loss: {:.4f}'.format(epoch_losses[-1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_labels(step):\n    return tensor(pd.get_dummies(train['grapheme_root'][(TRAIN_SIZE * step):(TRAIN_SIZE * (step + 1))]).values),\\\n           tensor(pd.get_dummies(train['vowel_diacritic'][(TRAIN_SIZE * step):(TRAIN_SIZE * (step + 1))]).values),\\\n           tensor(pd.get_dummies(train['consonant_diacritic'][(TRAIN_SIZE * step):(TRAIN_SIZE * (step + 1))]).values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.4. Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('train_0 is being processed')\nY_train_root, Y_train_vowel, Y_train_consonant = make_labels(0)\ntrain_nn(train_0, 0)\n\nY_train_root, Y_train_vowel, Y_train_consonant = make_labels(1)\ntrain_nn(train_0, 1)\n\nY_train_root, Y_train_vowel, Y_train_consonant = make_labels(2)\ntrain_nn(train_0, 2)\n\nY_train_root, Y_train_vowel, Y_train_consonant = make_labels(3)\ntrain_nn(train_0, 3)\n\nY_train_root, Y_train_vowel, Y_train_consonant = make_labels(4)\ntrain_nn(train_0, 4)\n\ndel train_0\n\nprint('train_1 is being processed')\ntrain_1 = pd.read_parquet('/kaggle/input/bengaliai-cv19/train_image_data_1.parquet')\n\nY_train_root, Y_train_vowel, Y_train_consonant = make_labels(5)\ntrain_nn(train_1, 0)\n\nY_train_root, Y_train_vowel, Y_train_consonant = make_labels(6)\ntrain_nn(train_1, 1)\n\nY_train_root, Y_train_vowel, Y_train_consonant = make_labels(7)\ntrain_nn(train_1, 2)\n\nY_train_root, Y_train_vowel, Y_train_consonant = make_labels(8)\ntrain_nn(train_1, 3)\n\nY_train_root, Y_train_vowel, Y_train_consonant = make_labels(9)\ntrain_nn(train_1, 4)\n\ndel train_1\n\nprint('train_2 is being processed')\ntrain_2 = pd.read_parquet('/kaggle/input/bengaliai-cv19/train_image_data_2.parquet')\n\nY_train_root, Y_train_vowel, Y_train_consonant = make_labels(10)\ntrain_nn(train_2, 0)\n\nY_train_root, Y_train_vowel, Y_train_consonant = make_labels(11)\ntrain_nn(train_2, 1)\n\nY_train_root, Y_train_vowel, Y_train_consonant = make_labels(12)\ntrain_nn(train_2, 2)\n\nY_train_root, Y_train_vowel, Y_train_consonant = make_labels(13)\ntrain_nn(train_2, 3)\n\nY_train_root, Y_train_vowel, Y_train_consonant = make_labels(14)\ntrain_nn(train_2, 4)\n\ndel train_2\n\nprint('train_3 is being processed')\ntrain_3 = pd.read_parquet('/kaggle/input/bengaliai-cv19/train_image_data_3.parquet')\n\nY_train_root, Y_train_vowel, Y_train_consonant = make_labels(15)\ntrain_nn(train_3, 0)\n\nY_train_root, Y_train_vowel, Y_train_consonant = make_labels(16)\ntrain_nn(train_3, 1)\n\nY_train_root, Y_train_vowel, Y_train_consonant = make_labels(17)\ntrain_nn(train_3, 2)\n\nY_train_root, Y_train_vowel, Y_train_consonant = make_labels(18)\ntrain_nn(train_3, 3)\n\nY_train_root, Y_train_vowel, Y_train_consonant = make_labels(19)\ntrain_nn(train_3, 4)\n\ndel train_3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.save(model.state_dict(), DEPLOYMENT_WEIGHTS)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. Evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_loss(epoch_losses):\n    plt.style.use('seaborn-whitegrid')\n    plt.figure()\n    \n    plt.plot(np.arange(0, N_EPOCHS * 20), epoch_losses)\n    \n    plt.title('Train Loss')\n    plt.xlabel('Epoch #')\n    plt.ylabel('Loss')\n    plt.show()    \n\nplot_loss(epoch_losses)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 6. Deployment"},{"metadata":{"trusted":true},"cell_type":"code","source":"#keras\n#prediction = model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_loader_test = data.DataLoader(\n    X_test,\n    batch_size=BATCH_SIZE,\n    num_workers=N_WORKERS,\n    shuffle=False\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.load_state_dict(torch.load(DEPLOYMENT_WEIGHTS))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results_graph, results_vowel, results_conso = [], [], []\nfor epoch in range(N_EPOCHS):\n    \n    print('Epoch {}/{}'.format(epoch, N_EPOCHS - 1))\n    print('-' * 10)\n        \n    model.eval()\n\n    for step, batch in enumerate(tqdm(data_loader_test)):\n        inputs = batch.to(device, dtype=torch.float)\n        \n        out_graph, out_vowel, out_conso = model(inputs)\n        \n        out_graph = F.softmax(out_graph, dim=1).data.cpu().numpy().argmax(axis=1)\n        out_vowel = F.softmax(out_vowel, dim=1).data.cpu().numpy().argmax(axis=1)\n        out_conso = F.softmax(out_conso, dim=1).data.cpu().numpy().argmax(axis=1)\n            \n            \n        results_graph.append(out_graph)\n        results_vowel.append(out_vowel)\n        results_conso.append(out_conso)\n            ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nresults = []\nresults_graph = pd.DataFrame(results_graph)\nresults_vowel = pd.DataFrame(results_vowel)\nresults_conso = pd.DataFrame(results_conso)\n\nfor arg in range(sample_submission.shape[0] // 3 ):\n    results.append(np.argmax(np.bincount(results_conso.iloc[:, arg])))\n    results.append(np.argmax(np.bincount(results_graph.iloc[:, arg])))\n    results.append(np.argmax(np.bincount(results_vowel.iloc[:, arg])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 7. Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"#torch\n\nsubmission = pd.concat([sample_submission.drop('target', axis=1), pd.Series(results)], names=['row_id', 'target'], axis=1)\nsubmission.rename(columns={0: 'target'}, inplace=True)\nsubmission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# keras\n\n# for pred_index, value in enumerate(prediction):\n#     for arg_index in range(3):\n#         sample_submission['target'].iloc[pred_index+(3*arg_index)] = np.argmax(value, axis=1)[arg_index]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}