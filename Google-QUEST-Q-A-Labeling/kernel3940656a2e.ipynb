{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Google Quest QA Labeling"},{"metadata":{},"cell_type":"markdown","source":"## 1. Data retrieval"},{"metadata":{},"cell_type":"markdown","source":"### 1.1. Import modules"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objs as go\nfrom matplotlib_venn import venn2\n\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn.linear_model import MultiTaskElasticNet\nfrom sklearn.preprocessing import minmax_scale\n\nimport json\nimport requests\nimport sys\nimport warnings\n\n\n# import tensorflow_hub as hub\nfrom transformers import DistilBertTokenizer, DistilBertModel\n# from transformers import BertTokenizer, BertModel\nfrom tqdm import tqdm\n\nfrom sklearn.model_selection import KFold\nfrom scipy.stats import spearmanr\n\nimport tensorflow as tf\nimport torch\nfrom keras.callbacks import Callback\nfrom keras.optimizers import Adam\nfrom keras.models import Model, Sequential\nfrom keras.layers import LSTM, Input, Dense, Dropout, Bidirectional, AveragePooling1D, BatchNormalization, AveragePooling2D, AveragePooling3D, MaxPooling1D, Conv1D, Conv2D\n\nwarnings.filterwarnings(\"ignore\")\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Offline models download"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install ../input/sacremoses/sacremoses-master/\n!pip install ../input/transformers/transformers-master/\n!ls ../input\n\n# add also bert-base-uncased (pytorch_model.bin, vocab.txt, config.json)to input dir (Add data)\nsys.path.insert(0, \"../input/transformers/transformers-master/\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1.2. Reading Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_set = pd.read_csv('../input/google-quest-challenge/train.csv')\nX_test = pd.read_csv('../input/google-quest-challenge/test.csv')\nsample_submission = pd.read_csv('../input/google-quest-challenge/sample_submission.csv')\nX_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_set = np.split(train_set, [train_set.columns.get_loc('question_asker_intent_understanding')], axis=1)\nX_train = train_set[0]\nX_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_train = pd.concat([X_train['qa_id'], train_set[1]], axis=1)\nY_train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Exploratory Data analysis"},{"metadata":{},"cell_type":"markdown","source":"### 2.1. Data Description"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Size of X_train', X_train.shape)\nprint('Size of Y_train', Y_train.shape)\nprint('Size of X_test', X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.2. Host distribution visualization"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_host_dist = X_train[\"host\"].value_counts()\nX_test_host_dist = X_test[\"host\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def host_distribution(distribution, title):\n    fig = px.pie(names=distribution.index, \n         values=distribution.values, \n         title=title,\n         width=800, \n         height=800)\n\n    fig.update_traces(textposition='inside', textinfo='percent+label')\n    fig.update_layout(showlegend=True)\n    fig.show()\n    \nhost_distribution(X_train_host_dist, 'Host data distribution on train data')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"host_distribution(X_test_host_dist, 'Host data distribution on test data')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.3. Categories distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"def categories_distribution(categories, title):\n    category_share = pd.DataFrame({'share': categories.value_counts() / categories.count()})\n    category_share['category'] = category_share.index   \n        \n    fig = px.bar(category_share, x='category', y='share',\n            labels={'share':'share in %'},\n            title=title)\n        \n    fig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categories_distribution(X_train['category'], 'X_train categories distribution barplot')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categories_distribution(X_test['category'], 'X_test categories distribution barplot')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.4. Common features in training and testing set (Venn diagrams)"},{"metadata":{"trusted":true},"cell_type":"code","source":"def venn_diagrams(columns, plot_num, title):\n    plt.subplot(plot_num)\n    venn2([set(columns[0].unique()), set(columns[1].unique())], set_labels = ('Train set', 'Test set'))\n    plt.title(title)\n    plt.show()\n    \nvenn_diagrams([X_train['question_user_name'], X_test['question_user_name']], 111, 'Common question_user_name in training and test data')\nvenn_diagrams([X_train['answer_user_name'], X_test['answer_user_name']], 121, 'Common answer_user_name in training and test data')\nvenn_diagrams([X_train['question_title'], X_test['question_title']], 131, 'Common question_title in training and test data')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.5. Length of contents"},{"metadata":{"trusted":true},"cell_type":"code","source":"def distribution_imposition(first_col, second_col, title):\n    plt.figure(figsize=(20, 6))\n    sns.distplot(first_col.str.len())\n    sns.distplot(second_col.str.len())\n    plt.title(title)\n    plt.show()\n        \ndistribution_imposition(X_train['question_title'], X_test['question_title'], 'Length of Question Title represented as cumulative distribution plots')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"distribution_imposition(X_train['question_body'], X_test['question_body'], 'Length of Question Body represented as cumulative distribution plots')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"distribution_imposition(X_train['answer'], X_test['answer'], 'Length of Question Body represented as cumulative distribution plots')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.6. Most popular questions"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.groupby('question_title').count()['qa_id'].sort_values(ascending=False).head(25)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train[X_train['question_title'] == 'What is the best introductory Bayesian statistics textbook?']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Feature Engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"class Feature_Engineering(object):\n    '''\n    Helper class used for Feature engineering purposes.\n    \n    '''\n    def __init__(self, dataframe):\n        self.dataframe = dataframe\n        with open(\"../input/charset/charset.json\", encoding=\"utf8\") as json_file:\n            self.charset  = json.load(json_file)\n        \n        self.vectorizer = TfidfVectorizer(ngram_range=(1, 3))\n        self.tsvd = TruncatedSVD(n_components = 128, n_iter=5)\n        \n        self.tokenizer = DistilBertTokenizer.from_pretrained('../input/distilbertbaseuncased/') \n        self.model = DistilBertModel.from_pretrained('../input/distilbertbaseuncased/')\n        \n#         self.tokenizer = BertTokenizer.from_pretrained('../input/bertlargeuncased/bert-large-uncased/')  \n#         self.model = BertModel.from_pretrained('../input/bertlargeuncased/bert-large-uncased/')\n        \n        self.binarizer = LabelBinarizer()\n\n\n    def unconstrained_chars(self, column, index):\n        df = self.dataframe[column][index]\n        for char in self.charset['CHARS']:\n            df = df.replace(char, '')\n        return df\n\n    def shortcuts_removal(self, column, index):\n        return ' '.join(list(map(lambda word: self.find_and_replace(word), self.dataframe[column][index].split())))\n        \n    def lower_case(self, column):\n        return self.dataframe[column].str.lower()\n        \n    \n    def find_and_replace(self, word):\n        for key, value in self.charset['SHORTCUTS'].items():\n            if key == word:\n                return value\n        return word\n\n    def flow(self, column):\n        self.dataframe[column] = self.lower_case(column)\n        for index in range(self.dataframe[column].shape[0]):\n            self.dataframe[column][index] = self.unconstrained_chars(column, index)\n            self.dataframe[column][index] = self.shortcuts_removal(column, index)\n\n        return self.dataframe[column]\n    \n    def tfidf_vec(self, column):\n        return list(self.tsvd.fit_transform(self.vectorizer.fit_transform(self.dataframe[column].values)))\n    \n    def binarize(self, column, other_df):\n        if len(self.dataframe[column].value_counts()) < len(other_df[column].value_counts()):\n            diff = abs(len(self.dataframe[column].value_counts()) - len(other_df[column].value_counts()))\n            return list(np.concatenate([list(self.binarizer.fit_transform(self.dataframe[column].values)), np.zeros((self.dataframe.shape[0], diff))], axis=1))\n        return list(self.binarizer.fit_transform(self.dataframe[column].values))\n    \n    def bert_separators(self, column):\n        for index in range(self.dataframe[column].shape[0]):\n            self.dataframe[column][index] = self.dataframe[column][index].split('.')\n            \n        return self.dataframe[column]\n    \n    def model_conf(self):\n        self.model.cpu()\n#         self.model.cuda()\n        \n    def make_vectors(self, column):\n        ids = self.dataframe[column].str.slice(0, 500).apply(self.tokenizer.encode)\n        vectors = []\n\n        for column in tqdm(ids):\n            input_ids = torch.Tensor(column).to(torch.int64).unsqueeze(0)\n            try:\n                outputs = self.model(input_ids.cpu())\n#                 outputs = self.model(input_ids.cuda())\n                vectors.append(outputs[0].detach().cpu().numpy().max(axis = 1))\n\n            except:\n                vectors.append(np.zeros(outputs[0].detach().cpu().numpy().max(axis = 1)).shape)\n        \n        return vectors","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.1. Data Cleaning"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features = Feature_Engineering(X_train)\ntest_features = Feature_Engineering(X_test)\n\nX_train['question_title'] = train_features.flow('question_title')\nX_train['question_body'] = train_features.flow('question_body')\nX_train['answer'] = train_features.flow('answer')\nX_test['question_title'] = test_features.flow('question_title')\nX_test['question_body'] = test_features.flow('question_body')\nX_test['answer'] = test_features.flow('answer')\n\nX_train.drop(columns=['question_user_page', 'answer_user_page', 'url'], inplace=True)\nX_test.drop(columns=['question_user_page', 'answer_user_page', 'url'], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.2. Transform frequent operations on documents"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train['question_title_tfidf_vec'] = train_features.tfidf_vec('question_title')\nX_train['question_body_tfidf_vec'] = train_features.tfidf_vec('question_body')\nX_train['answer_tfidf_vec'] = train_features.tfidf_vec('answer')\n\nX_test['question_title_tfidf_vec'] = test_features.tfidf_vec('question_title')\nX_test['question_body_tfidf_vec'] = test_features.tfidf_vec('question_body')\nX_test['answer_tfidf_vec'] = test_features.tfidf_vec('answer')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.3. Encoding categorical features"},{"metadata":{"trusted":true},"cell_type":"code","source":"# X_train['category_vec'] = list(LabelBinarizer().fit_transform(X_train['category'].values))\n# X_train['question_user_name_vec'] = list(LabelBinarizer().fit_transform(X_train['question_user_name'].values))\n# X_train['answer_user_name_vec'] = list(LabelBinarizer().fit_transform(X_train['answer_user_name'].values))\n# X_train['host_vec'] = list(LabelBinarizer().fit_transform(X_train['host'].values))\n\n# diff = abs(len(X_test['question_user_name'].value_counts()) - len(X_train['question_user_name'].value_counts()))\n# d = np.zeros((X_test.shape[0], diff))\n# a = list(LabelBinarizer().fit_transform(X_train['question_user_name'].values))\n# b = list(LabelBinarizer().fit_transform(X_test['question_user_name'].values))\n# a = np.array(a)\n# c = np.concatenate([b, d], axis=1)\n# print(np.array(a).shape)\n# print(np.array(b).shape)\n# print(np.array(c).shape)\n\nX_train['category_vec'] = train_features.binarize('category', X_test)\nX_train['question_user_name_vec'] = train_features.binarize('question_user_name', X_test)\nX_train['answer_user_name_vec'] = train_features.binarize('answer_user_name', X_test)\nX_train['host_vec'] = train_features.binarize('host', X_test)\n\nX_test['category_vec'] = test_features.binarize('category', X_train)\nX_test['question_user_name_vec'] = test_features.binarize('question_user_name', X_train)\nX_test['answer_user_name_vec'] = test_features.binarize('answer_user_name', X_train)\nX_test['host_vec'] = test_features.binarize('host', X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.4. Bidirectional Encoder Representations from Transformers (BERT)"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features.model_conf()\ntest_features.model_conf()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train['question_title'] = train_features.bert_separators('question_title')\nX_train['question_body'] = train_features.bert_separators('question_body')\nX_train['answer'] = train_features.bert_separators('answer')\n\nX_test['question_title'] = test_features.bert_separators('question_title')\nX_test['question_body'] = test_features.bert_separators('question_body')\nX_test['answer'] = test_features.bert_separators('answer')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"question_title_vectors_train = train_features.make_vectors(\"question_title\")\nquestion_body_vectors_train = train_features.make_vectors(\"question_body\")\nanswer_vectors_train = train_features.make_vectors(\"answer\")\n\nquestion_title_vectors_test = test_features.make_vectors(\"question_title\")\nquestion_body_vectors_test = test_features.make_vectors(\"question_body\")\nanswer_vectors_test = test_features.make_vectors(\"answer\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nX_train_1 = np.concatenate([\n                     #np.vstack(X_train['category_vec']),\n                     #np.vstack(X_train['host_vec']),\n    \n                     np.array(question_title_vectors_train)[:,0,:],\n                     #np.vstack(X_train['question_title_tfidf_vec']),\n                     np.array(question_body_vectors_train)[:,0,:],\n                     #np.vstack(X_train['question_body_tfidf_vec']),\n                     # np.vstack(X_train['question_user_name_vec']),\n    \n                     np.array(answer_vectors_train)[:,0,:],\n                     #np.vstack(X_train['answer_tfidf_vec'])\n                     # np.vstack(X_train['answer_user_name_vec'])\n                     ], axis = 1)\n\nX_train_2 = np.concatenate([\n                     #np.vstack(X_train['category_vec']),\n                     #np.vstack(X_train['host_vec']),\n    \n                     #np.array(question_title_vectors_train)[:,0,:],\n                     np.vstack(X_train['question_title_tfidf_vec']),\n                     #np.array(question_body_vectors_train)[:,0,:],\n                     np.vstack(X_train['question_body_tfidf_vec']),\n                     # np.vstack(X_train['question_user_name_vec']),\n    \n                     #np.array(answer_vectors_train)[:,0,:],\n                     np.vstack(X_train['answer_tfidf_vec'])\n                     # np.vstack(X_train['answer_user_name_vec'])\n                     ], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"                     \nX_test_1 = np.concatenate([\n                     #np.vstack(X_test['category_vec']),\n                     # np.vstack(X_test['host_vec']),\n    \n                     np.array(question_title_vectors_test)[:,0,:],\n                     #np.vstack(X_test['question_title_tfidf_vec']),\n                     np.array(question_body_vectors_test)[:,0,:],\n                     #np.vstack(X_test['question_body_tfidf_vec']),\n                     # np.vstack(X_test['question_user_name_vec']),\n    \n                     np.array(answer_vectors_test)[:,0,:]\n                     #np.vstack(X_test['answer_tfidf_vec'])\n                     # np.vstack(X_test['answer_user_name_vec'])\n                     ], axis = 1)\n\nX_test_2 = np.concatenate([\n                     #np.vstack(X_test['category_vec']),\n                     #np.vstack(X_test['host_vec']),\n    \n                     #np.array(question_title_vectors_test)[:,0,:],\n                     np.vstack(X_test['question_title_tfidf_vec']),\n                     #np.array(question_body_vectors_test)[:,0,:],\n                     np.vstack(X_test['question_body_tfidf_vec']),\n                     # np.vstack(X_test['question_user_name_vec']),\n    \n                     #np.array(answer_vectors_test)[:,0,:],\n                     np.vstack(X_test['answer_tfidf_vec'])\n                     # np.vstack(X_test['answer_user_name_vec'])\n                     ], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_train_1.shape)\nprint(X_train_2.shape)\nprint(X_test_1.shape)\nprint(X_test_2.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_1 = np.array(minmax_scale(pd.DataFrame(X_train_1)))\nX_train_2 = np.array(minmax_scale(pd.DataFrame(X_train_2)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test_1 = np.array(minmax_scale(pd.DataFrame(X_test_1)))\nX_test_2 = np.array(minmax_scale(pd.DataFrame(X_test_2)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Modeling and training"},{"metadata":{"trusted":true},"cell_type":"code","source":"class SpearmanCallback(Callback):\n    '''\n    Class for calculating and displaing Spearman Rho value\n    as a callback after each epoch.\n    '''\n    def __init__(self, validation_data, model_name, model):\n        self.x_val = validation_data[0]\n        self.y_val = validation_data[1]\n        \n        self.model_name = model_name\n        self.model = model\n        \n    def on_epoch_end(self, epoch, logs={}):\n        y_pred_val = self.model.predict(self.x_val)\n        rho_val = None\n        if model=='LSTM':\n            rho_val = np.mean([spearmanr(np.array(self.y_val)[:, 0, ind], np.array(y_pred_val)[:, 0, ind]).correlation for ind in range(y_pred_val.shape[1])])\n        else:\n            rho_val = np.mean([spearmanr(np.array(self.y_val)[:, ind], np.array(y_pred_val)[:, ind]).correlation for ind in range(y_pred_val.shape[1])])\n            \n        #self.model.save_weights(self.model_name)\n        if np.isnan(rho_val):\n            return -1\n        else:\n            print('validation rho spearman callback value: {}'.format(rho_val))\n            return rho_val","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef create_model(model):\n    x = None\n    \n    if model=='LSTM':\n        inps = Input(shape=(1, X_train_1.shape[1]))\n        x = Bidirectional(LSTM(512, dropout=0.2, return_sequences=True))(inps)\n        #x = AveragePooling1D(pool_size=2, data_format='channels_first')(x)\n        x = BatchNormalization()(x)\n        x = Bidirectional(LSTM(512, dropout=0.2, return_sequences=True))(x)\n    else:\n        inps = Input(shape=(X_train_2.shape[1],))\n        x = Dense(1024, activation='elu')(inps)\n        x = Dropout(0.2)(x)\n        x = BatchNormalization()(x)\n        x = Dense(1024, activation='elu')(x)\n        \n        \n    y = Dense(Y_train.shape[1], activation='sigmoid')(x)\n    model = Model(inputs=inps, outputs=y)\n\n    model.compile(\n        optimizer=Adam(learning_rate=0.0001), # to check\n        loss=['binary_crossentropy'] #'mean_squared_error'\n    )\n    model.summary()\n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.1. Model Stacking BERT > LSTM"},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions_lstm = []\n\nkf = KFold(n_splits=5, random_state=42, shuffle=True)\n\nX_test_tmp = X_test_1.reshape(-1, 1, X_test_1.shape[1])\n\nfor ind, (train, validation) in enumerate(kf.split(X_train_1)):\n    X_train_part= X_train_1[train]\n    Y_train_part = Y_train.iloc[train, :]\n    X_validate_part = X_train_1[validation]\n    Y_validate_part = Y_train.iloc[validation, :]\n    \n    X_train_part = X_train_part.reshape(-1, 1, X_train_1.shape[1])\n    Y_train_part = np.array(Y_train_part).reshape(-1, 1, Y_train.shape[1])\n    X_validate_part = X_validate_part.reshape(-1, 1, X_validate_part.shape[1])\n    Y_validate_part = np.array(Y_validate_part).reshape(-1, 1, Y_validate_part.shape[1])\n    \n    model = create_model('LSTM')\n    model.fit(X_train_part, Y_train_part, epochs=20, batch_size=32, validation_data=(X_validate_part, Y_validate_part), verbose=True,\n        callbacks=[SpearmanCallback(validation_data=(X_validate_part, Y_validate_part), model_name=f'best_model_batch{ind}.h5', model='LSTM')])\n    predictions_lstm.append(model.predict(X_test_tmp))\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.2. Model Stacking TFIDF > Dense"},{"metadata":{"trusted":true},"cell_type":"code","source":"\npredictions_dense = []\n\nfor ind, (train, validation) in enumerate(kf.split(X_train_2)):\n    X_train_part= X_train_2[train]\n    Y_train_part = Y_train.iloc[train, :]\n    X_validate_part = X_train_2[validation]\n    Y_validate_part = Y_train.iloc[validation, :]\n    \n    model = create_model('DENSE')\n    model.fit(X_train_part, Y_train_part, epochs=20, batch_size=32, validation_data=(X_validate_part, Y_validate_part), verbose=True,\n        callbacks=[SpearmanCallback(validation_data=(X_validate_part, Y_validate_part), model_name=f'best_model_batch{ind}.h5', model='DENSE')])\n    predictions_dense.append(model.predict(X_test_2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.3. Model Stacking BERT > MultiTaskElasticNet"},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions_mt = []\nfor ind, (train, val) in enumerate(kf.split(X_train)):\n    X_train_part= X_train_1[train]\n    Y_train_part = Y_train.iloc[train, :]\n\n    model = MultiTaskElasticNet(alpha=0.00001, max_iter=100, random_state=42)\n    model.fit(X_train_part, Y_train_part)\n    predictions_mt.append(model.predict(X_test_1))","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"all_predictions = np.concatenate([np.array(predictions_lstm)[:,:,0,:], np.array(predictions_dense), np.array(predictions_mt)])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.4. Averaging predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"final = pd.DataFrame(np.array(all_predictions)[0,:,:])\nfinal = pd.concat([pd.DataFrame(np.array(all_predictions)[valid_pred,:,:]) for valid_pred in range(15)], axis=1)\nfor num in range(31):\n    final[num] = final.iloc[:, [num * valid_pred for valid_pred in range(15)]].mean(axis=1)\nfinal = final.iloc[:, :31]\nfinal = pd.DataFrame(minmax_scale(final))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.5. Submiting "},{"metadata":{"trusted":true},"cell_type":"code","source":"final.columns = sample_submission.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final['qa_id'] = sample_submission['qa_id']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":1}